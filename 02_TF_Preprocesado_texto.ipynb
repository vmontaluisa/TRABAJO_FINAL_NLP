{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1K05ylR3wj9HmgEtBeU03RtE3az9bLImH","timestamp":1733720891870}],"authorship_tag":"ABX9TyPHq+gtHDadh1pAdoCQGzKG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"lCKeR5ae0PFB","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1733721378940,"user_tz":300,"elapsed":55668,"user":{"displayName":"Victor Montaluisa","userId":"05430985851832793631"}},"outputId":"b2db7959-9f72-4179-8731-098e51c4b230"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting num2words\n","  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n","Collecting docopt>=0.6.2 (from num2words)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: docopt\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=2daee92c45131b5eb29804905930f534a5b33bb9fa93b4014d5e33ff92cb3fd1\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built docopt\n","Installing collected packages: docopt, num2words\n","Successfully installed docopt-0.6.2 num2words-0.5.13\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Requirement already satisfied: WordCloud in /usr/local/lib/python3.10/dist-packages (1.9.4)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from WordCloud) (1.26.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from WordCloud) (11.0.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from WordCloud) (3.8.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->WordCloud) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->WordCloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->WordCloud) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->WordCloud) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->WordCloud) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->WordCloud) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->WordCloud) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->WordCloud) (1.16.0)\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Collecting Counter\n","  Downloading Counter-1.0.0.tar.gz (5.2 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: Counter\n","  Building wheel for Counter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Counter: filename=Counter-1.0.0-py3-none-any.whl size=5394 sha256=a24f7c5bfb6f97f4fe84d338896fe9f69af2405902a432b7e854526646fd2eb6\n","  Stored in directory: /root/.cache/pip/wheels/e3/02/6d/d5c0838427a060718c6060ae4d24da95a0e0df0d7a3dab8040\n","Successfully built Counter\n","Installing collected packages: Counter\n","Successfully installed Counter-1.0.0\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Collecting Word2Vec\n","  Downloading word2vec-0.11.1.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from Word2Vec) (1.26.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from Word2Vec) (1.4.2)\n","Building wheels for collected packages: Word2Vec\n","  Building wheel for Word2Vec (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Word2Vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=141246 sha256=6f8da7544f6577dbe020e41e6e0cc5567a7e830270265be17411009b167c135a\n","  Stored in directory: /root/.cache/pip/wheels/6a/fa/d1/e03e8c10e0e2aa5c7b6e2b46b4a1c715d140283853937bb4b1\n","Successfully built Word2Vec\n","Installing collected packages: Word2Vec\n","Successfully installed Word2Vec-0.11.1\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"]}],"source":["!pip install num2words\n","!pip install nltk\n","!pip install pip beautifulsoup4\n","!pip install pip WordCloud\n","!pip install pip Counter\n","!pip install pip Word2Vec\n","!pip install scikit-learn\n","!pip install gensim"]},{"cell_type":"code","source":["import pandas as pd\n","import unicodedata\n","from num2words import num2words\n","from nltk import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from wordcloud import WordCloud\n","from collections import Counter\n","from sklearn.manifold import TSNE\n","from gensim.models import Word2Vec\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","from nltk import ngrams\n","\n","from collections import Counter\n","import re\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","nltk_sw_list = stopwords.words('english')\n","nltk_lematizer_list = WordNetLemmatizer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CZHpMOJ2BoQ","executionInfo":{"status":"ok","timestamp":1733722684028,"user_tz":300,"elapsed":402,"user":{"displayName":"Victor Montaluisa","userId":"05430985851832793631"}},"outputId":"d6dafe1d-10c6-4ac6-8aae-415ca6809e85"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]}]},{"cell_type":"code","source":["# Funcion para Preprocesado\n","\n","\n","def eliminar_stop_words(texto):\n","    tokens = word_tokenize(texto)\n","    tokens = [word for word in tokens if word not in nltk_sw_list]\n","    return tokens\n","\n","def eliminar_signos_puntuacion(texto):\n","    text = re.sub(r'[^a-z\\s]', '', texto)\n","    return text\n","\n","def eliminar_casos_especiales(texto):\n","    texto= texto.replace('<br />',' ')\n","    return texto\n","\n","def elimimnar_etiquetas_html(texto):\n","   text = BeautifulSoup(texto, \"html.parser\").get_text()\n","   texto = re.sub(r'<.*?>', '', texto)\n","   return texto\n","\n","def revision_generica(text):\n","    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","    text = text.lower()\n","    return text\n","\n","\n","def revision_lematizer(tokens):\n","    tokens = [nltk_lematizer_list.lemmatize(word) for word in tokens]\n","    return tokens\n","\n","\n","def texto_preproceado(text):\n","    clean_text = list()\n","\n","    text = revision_generica(text)\n","    text = elimimnar_etiquetas_html( text)\n","    text = eliminar_signos_puntuacion(text)\n","    text = eliminar_casos_especiales(text)\n","    tokens = eliminar_stop_words(text)\n","    tokens = revision_lematizer(tokens)\n","\n","    clean_text = ' '.join(tokens)\n","\n","    return clean_text"],"metadata":{"id":"SabqV1j-2OfT","executionInfo":{"status":"ok","timestamp":1733724729886,"user_tz":300,"elapsed":150,"user":{"displayName":"Victor Montaluisa","userId":"05430985851832793631"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["# Prueba\n","texto_prueba = \"This product is <b>amazing</b>! I used<br> it for <br /> two , and. loved it! https://example.com\"\n","texto_resultado = texto_preproceado(texto_prueba)\n","print(\"Resutlado:\", texto_resultado)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5_QajgJ6VWk","executionInfo":{"status":"ok","timestamp":1733725634305,"user_tz":300,"elapsed":156,"user":{"displayName":"Victor Montaluisa","userId":"05430985851832793631"}},"outputId":"c8953b4c-1d75-4600-fa27-a50ae0009120"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Resutlado: product amazing used two loved httpsexamplecom\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zRzBxK29-S6o"},"execution_count":null,"outputs":[]}]}